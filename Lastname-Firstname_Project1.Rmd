---
title: "Lastname-Firstname_Project1"
author:
- Guillermo Delgado
- Katie Guillen
- Leanne Harper
- Nicolas Schoonmaker
date: "1/25/2020"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

# Setup
## Using RMD from: https://github.com/wgfoote/fin-alytics/blob/master/RMD/PR01-Rstartup-HO2.Rmd

## Our code is on github at: https://github.com/nschoonm/FIN654-Project1

## Disable inline preview of the Markdown file:
### In RStudio, Tools > Global Options... > R Markdown > Show equations and Image previews (Never)

## Setup chunk defaults
```{r Chunk setup}
require(knitr)
# Echo the output
knitr::opts_chunk$set(echo = TRUE)
```

## Display packages to install
```{r Packages and Installers}
# Install RTools
# https://cran.rstudio.com/bin/windows/Rtools/
#
# Install Tinytex
# tinytex::install_tinytex()
#
# Restart R Studio
#
# Install packages
#install.packages("xtable")
#install.packages("dplyr")
#install.packages("rstudioapi")
#install.packages("tinytex")
```

## Show libraries to include
```{r Library Includes}
# The list of libraries to include
library(stats)
library(xtable)
library(dplyr)
library(rstudioapi)
```

## Get the currenet working directory so we can run this file from anywhere. This will allow this script to work from RStudio or from the command line
```{r Get current directory}
# Get the directory so we can run this from anywhere
# Get the script directory from R when running in R
if(rstudioapi::isAvailable())
{
  print("Running in RStudio")
  script.path <- rstudioapi::getActiveDocumentContext()$path
  (script.path)
  script.dir <- dirname(script.path)
}
if(!exists("script.dir"))
{
  print("Running from command line")
  script.dir <- getSrcDirectory(function(x) {x})
}
(script.dir)
```

## Set the working directory based on where the script is
```{r Working Directory and Data setup}
# Set my working directory
# There is a "data" folder here with the files and the script
setwd(script.dir)
# Double check the working directory
getwd()
# Error check to ensure the working directory is set up and the data
# directory exists inside it.  Its required for this file
if(dir.exists(paste(getwd(),"/data", sep = "")) == FALSE) {
  stop("Data directory does not exist. Make sure the working directory
       is set using setwd() and the data folder exists in it.")
} else {
  print("Working directory and data set up correctly")
}
```

## Some example help commands
```{r View some command info}
# See some help command info (examples)
#??read.csv
#??head
#??na.omit
```

# Part 1

## Introduction

### In this set we will build a data set using filters and `if` and `diff` statements. We will then answer some questions using plots and a pivot table report. We will then write a function to house our approach in case we would like to run the same analysis on other data sets.

## Problem

### Supply chain managers at our company continue to note we have a significant exposure to heating oil prices (Heating Oil No. 2, or HO2), specifically New York Harbor. The exposure hits the variable cost of producing several products. When HO2 is volatile, so is earnings. Our company has missed earnings forecasts for five straight quarters. To get a handle on HO2 we download this data set and review some basic aspects of the prices. 

```{r Read in CSV data}
# Read in data
# package EIAdata
#
HO2 <- read.csv("data/nyhh02.csv", header = T, stringsAsFactors = F)
# stringsAsFactors sets dates as character type
head(HO2)
head(HO2, n = 10)
tail(HO2, n = 10)
HO2 <- na.omit(HO2) ## to clean up any missing data
# use na.approx() as well
str(HO2) # review the structure of the data so far
```

## Questions

### 1. What is the nature of HO2 returns? We want to reflect the ups and downs of price movements, something of prime interest to management. First, we calculate percentage changes as log returns. Our interest is in the ups and downs. To look at that we use `if` and `else` statements to define a new column called `direction`. We will build a data frame to house this analysis.

```{r Construct Data frame}
# Construct expanded data frame
# Euler
return <- as.numeric(diff(log(HO2$DHOILNYH))) * 100  
# size is indicator of volatility
size <- as.numeric(abs(return)) 
# another indicator of volatility
direction <- ifelse(return > 0, "up", ifelse(return < 0, "down", "same")) 
# =if(return > 0, "up", if(return < 0, "down", "same"))
# length of DATE is length of return +1: omit 1st observation
date <- as.Date(HO2$DATE[-1], "%m/%d/%Y") 
# length of DHOILNYH is length of return +1: omit first observation
price <- as.numeric(HO2$DHOILNYH[-1]) 
# clean up data frame by omitting NAs
HO2.df <- na.omit(data.frame(date = date, 
                             price = price, 
                             return = return, 
                             size = size, 
                             direction = direction)) 
```

```{r View data size and content}
str(HO2.df)
nrow(HO2.df) # num data points
ncol(HO2.df) # 5, date, price, return, size, direction
dim(HO2.df) # both, num rows then num cols
hist(price) # histogram the price
axis(side=1, at=seq(0,4,1), labels=seq(0,4,1))
``` 

#### We can plot with the `ggplot2` package. In the `ggplot` statements we use `aes`, "aesthetics", to pick `x` (horizontal) and `y` (vertical) axes. Use `group =1` to ensure that all data is plotted. The added (`+`) `geom_line` is the geometrical method that builds the line plot.

```{r Plot line graph}
library(ggplot2)
p <- ggplot(HO2.df, 
            aes(x = date, y = return, group = 1)) +
  geom_line(colour = "blue")
p
``` 

#### Let's try a bar graph of the absolute value of price rates. We use `geom_bar` to build this picture.

```{r Plot bar graph}
# library(ggplot2)
p <- ggplot(HO2.df, 
            aes(x = date, y = size, group = 1)) +
  geom_bar(stat = "identity", colour = "green")
p
```

#### Now let's build an overlay of `return` on `size` below.


```{r Plot line and bar graph, fig.align='center'}
p <- ggplot(HO2.df, aes(date, size)) + 
  geom_bar(stat = "identity", colour = "darkorange") +
  geom_line(data = HO2.df, aes(date, return), colour = "blue")
p
```

### 2. Let's dig deeper and compute mean, standard deviation, etc. Load the `data_moments()` function. Run the function using the `HO2.df$return` subset of the data and write a `knitr::kable()` report.

```{r Create data_moments function and run it}
# Load the data_moments() function
## data_moments function
## INPUTS: vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  mean.r <- mean(data)
  sd.r <- sd(data)
  median.r <- median(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, 
                       std_dev = sd.r,
                       median = median.r, 
                       skewness = skewness.r,
                       kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(HO2.df$return)
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
```

### 3. Let's pivot `size` and `return` on `direction`. What is the average and range of returns by direction? How often might we view positive or negative movements in HO2?

```{r Create pivot table}
# Counting
table(HO2.df$return < 0) # one way
table(HO2.df$return > 0)
table(HO2.df$direction) # this counts 0 returns as negative
table(HO2.df$return == 0)
# Pivoting
library(dplyr)
## 1: filter to those houses with fairly high prices
pivot.table <- dplyr::filter(HO2.df, size > 0.5*max(size))
## 2: set up data frame for by-group processing
pivot.table <-  group_by(HO2.df, direction)
## 3: calculate the summary metrics
options(dplyr.width = Inf) ## to display all columns
HO2.count <- length(HO2.df$return)
pivot.table <-  summarise(pivot.table, 
                          return.avg = round(mean(return), 4),
                          return.sd = round(sd(return), 4), 
                          quantile.5 = round(quantile(return, 0.05), 4),
                          quantile.95 = round(quantile(return, 0.95), 4),
                          percent = 
                            round((length(return)/HO2.count)*100, 2))
# Build visual
knitr::kable(pivot.table, digits = 2)
# Here is how we can produce a LaTeX formatted and rendered table
# 
library(xtable)
HO2.caption <- "Heating Oil No. 2: 1986-2016"
print(xtable(t(pivot.table), 
             digits = 2, 
             caption = HO2.caption,
             align=rep("r", 4), 
             table.placement="V"))
knitr::kable(t(pivot.table), 
             digits = 2, 
             caption = HO2.caption,
             align=rep("r", 4), 
             table.placement="V")
print(xtable(answer), digits = 2)
knitr::kable(answer, digits = 2)
```

# Part 2

## Introduction

### We will use the data from Part 1 to investigate the distribution of returns we generated. This will entail fitting the data to some parametric distributions as well as 

## Problem

### We want to further characterize the distribution of up and down movements visually. Also we would like to repeat the analysis periodically for inclusion in management reports.

## Questions

### 1. How can we show the differences in the shape of ups and downs in HO2, especially given our tolerance for risk? We can use the `HO2.df` data frame with `ggplot2` and the cumulative relative frequency function `stat_ecdf` to begin to understand this data.

```{r Plot data to observe shape}
HO2.tol.pct <- 0.95
HO2.tol <- quantile(HO2.df$return, HO2.tol.pct)
HO2.tol.label <- paste("Tolerable Rate = ", round(HO2.tol, 2), sep = "")
ggplot(HO2.df, 
       aes(return, fill = direction)) + 
  stat_ecdf(colour = "blue", size = 0.75) + 
  geom_vline(xintercept = HO2.tol, colour = "red", size = 1.5) +
  annotate("text", x = HO2.tol+5 , 
           y = 0.75, label = HO2.tol.label, colour = "darkred")
```

### 2. How can we regularly, and reliably, analyze HO2 price movements? For this requirement, let's write a function similar to `data_moments`. Name this new function `HO2_movement()`.

```{r Create HO2_movement function}
## HO2_movement(file, caption)
## input: HO2 csv file from /data directory
## output: result for input to kable in $table and xtable in $xtable; 
##         data frame for plotting and further analysis in $df.
## Example: HO2.data <- HO2_movement(file = "data/nyhh02.csv", caption = "HO2 NYH")
HO2_movement <- function(file = "data/nyhh02.csv",
                         caption = "Heating Oil No. 2: 1986-2016"){
  # Read file and deposit into variable
  HO2 <- read.csv(file, header = T, stringsAsFactors = F)
  # stringsAsFactors sets dates as character type
  HO2 <- na.omit(HO2) ## to clean up any missing data
  # Construct expanded data frame
  return <- as.numeric(diff(log(HO2$DHOILNYH))) * 100
  # size is indicator of volatility
  size <- as.numeric(abs(return)) 
  # another indicator of volatility
  direction <- ifelse(return > 0, "up", ifelse(return < 0, "down", "same")) 
  # length of DATE is length of return +1: omit 1st observation
  date <- as.Date(HO2$DATE[-1], "%m/%d/%Y") 
  # length of DHOILNYH is length of return +1: omit first observation
  price <- as.numeric(HO2$DHOILNYH[-1]) 
  # clean up data frame by omitting NAs
  HO2.df <- na.omit(data.frame(date = date, 
                               price = price, 
                               return = return, 
                               size = size, 
                               direction = direction)) 
  require(dplyr)
  ## 1: filter if necessary
  # pivot.table <- dplyr::filter(HO2.df, size > 0.5*max(size))
  ## 2: set up data frame for by-group processing
  pivot.table <-  group_by(HO2.df, direction)
  ## 3: calculate the summary metrics
  options(dplyr.width = Inf) ## to display all columns
  HO2.count <- length(HO2.df$return)
  pivot.table <-  summarise(pivot.table, 
                            return.avg = mean(return), 
                            return.sd = sd(return), 
                            quantile.5 = quantile(return, 0.05), 
                            quantile.95 = quantile(return, 0.95), 
                            percent = (length(return)/HO2.count)*100)
  # Construct transpose of pivot table with xtable()
  require(xtable)
  pivot.xtable <- xtable(t(pivot.table), 
                         digits = 2, 
                         caption = caption, 
                         align=rep("r", 4), 
                         table.placement="V")
  HO2.caption <- "Heating Oil No. 2: 1986-2016"
  output.list <- list(table = pivot.table, 
                      xtable = pivot.xtable, 
                      df = HO2.df)
return(output.list)
}
```

#### Test `HO2_movement()` with data and display results in a table with `2` decimal places.
```{r Run HO2_movement function}
knitr::kable(HO2_movement(file = "data/nyhh02.csv")$table, digits = 2)
```

#### Morale: more work today (build the function) means less work tomorrow (write yet another report).

### 3. Suppose we wanted to simulate future movements in HO2 returns. What distribution might we use to run those scenarios? Here, let's use the `MASS` package's `fitdistr()` function to find the optimal fit of the HO2 data to a parametric distribution. We will use the `gamma` distribution to simulate future heating oil \#2 price scenarios.

```{r Observe the distribution}
library(MASS)
HO2.data <- HO2_movement(file = "data/nyhh02.csv", 
                         caption = "HO2 NYH")$df
str(HO2.data)
fit.gamma.up <- fitdistr(HO2.data[HO2.data$direction == "up", "return"],
                         "gamma", hessian = TRUE, lower=c(-1, 0))
fit.gamma.up
# a problem here is all observations = 0
# fit.t.same <- fitdistr(HO2.data[HO2.data$direction == "same", "return"],
#                       "gamma", hessian = TRUE) 
fit.t.down <- fitdistr(HO2.data[HO2.data$direction == "down", "return"],
                       "t", hessian = TRUE, lower=c(-1, 0))
fit.t.down
# gamma distribution defined for data >= 0
fit.gamma.down <- 
  fitdistr(-HO2.data[HO2.data$direction == "down", "return"],
           "gamma", hessian = TRUE )
fit.gamma.down
```

# Conclusion

## Skills and Tools

The following methods & packages in R were used to explore the data:

1. We used "if" and "else" statements to define a new column called "direction" and then created a data frame to house this analysis.
2. We used the ggplot package to plot our data so that we could visualize the size and returns
3. We used the data moments function to find the standard deviation, mean, skewness and kurtosis
4. We used the knitr package to format the data moments function into a table
5. We used the table function to pivot size and return on direction
6. We used the dplyr to filter to show results for how often we might observe fluctuations in the movements of HO2
7. We used the MASS package to find the optimal fit of the HO2 data
8. We also used our statistical analysis methods to summarize and draw conclusions from the data

The specific functions used where:

1. read.csv
  + Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file. read.csv and read.csv2 are identical to read.table except for the defaults. They are intended for reading ‘comma separated value’ files (‘.csv’).
2. head
  + Returns the first or last parts of a vector, matrix, table, data frame or function. Since head() and tail() are generic functions, they may also have been extended to other classes.
3. tail
  + Returns the first (last) n rows when n >= 0 or all but the last (first) n rows when n < 0.
4. na.omit
  + Returns the object with incomplete cases removed.
5. str
  + Compactly display the internal structure of an R object, a diagnostic function and an alternative to summary 
6. as.numeric(diff(log(HO2$DHOILNYH))) * 100
  + Used to determine the Euler value
7. data.frame
  + Creates data frames, tightly coupled collections of variables which share many of the properties of matrices and of lists, used as the fundamental data structure by most of R's modeling software.
8. ggplot
  + Initializes a ggplot object. It can be used to declare the input data frame for a graphic and to specify the set of plot aesthetics intended to be common throughout all subsequent layers unless specifically overridden.
9. aes
  + Aesthetic mappings describe how variables in the data are mapped to visual properties (aesthetics) of geoms.
10. geom_line
  + Connects observations in order of the variable on the x axis. 
11. geom_bar
  + Makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights).
12. mean
  + Generic function for the (trimmed) arithmetic mean.
13. sd
  + This function computes the standard deviation of the values in x.
14. median
  + Compute the sample median.
15. skewness
  + This function computes skewness of given data. In statistics, skewness is a measure of the asymmetry of the probability distribution of a random variable about its mean. In other words, skewness tells you the amount and direction of skew (departure from horizontal symmetry). The skewness value can be positive or negative, or even undefined.
16. kurtosis
  + This function computes the estimator of Pearson's measure of kurtosis. Like skewness, kurtosis is a statistical measure that is used to describe the distribution. Whereas skewness differentiates extreme values in one versus the other tail, kurtosis measures extreme values in either tail. Distributions with large kurtosis exhibit tail data exceeding the tails of the normal distribution (e.g., five or more standard deviations from the mean). Distributions with low kurtosis exhibit tail data that are generally less extreme than the tails of the normal distribution.
17. round
  + Round rounds the values in its first argument to the specified number of decimal places (default 0).
18. knitr::kable
  + This is a very simple table generator. It is simple by design. It is not intended to replace any other R packages for making tables.
19. dplyr::filter
  + Choose rows/cases where conditions are true. Unlike base subsetting with [, rows where the condition evaluates to NA are dropped.
20. group_by
  + Takes an existing tbl and converts it into a grouped tbl where operations are performed "by group".
21. length
  + Get or set the length of vectors (including lists) and factors, and of any other R object for which a method has been defined.
22. summarise
  + Create one or more scalar variables summarizing the variables of an existing tbl. Tbls with groups created by group_by() will result in one row in the output for each group. Tbls with no groups will result in one row.
23. xtable
  + Convert an R object to an xtable object, which can then be printed as a LaTeX or HTML table.
24. quantile
  + The generic function quantile produces sample quantiles corresponding to the given probabilities. The smallest observation corresponds to a probability of 0 and the largest to a probability of 1.
25. stat_ecdf
  + The empirical cumulative distribution function (ECDF) provides an alternative visualisation of distribution.
26. geom_vline
  + These geoms add reference lines (sometimes called rules) to a plot, either horizontal, vertical, or diagonal (specified by slope and intercept). These are useful for annotating plots.
27. annotate
  + This function adds geoms to a plot, but unlike typical a geom function, the properties of the geoms are not mapped from variables of a data frame, but are instead passed in as vectors. This is useful for adding small annotations (such as text labels) or if you have your data in vectors, and for some reason don't want to put them in a data frame.
28. fitdistr
  + Maximum-likelihood fitting of univariate distributions, allowing parameters to be held fixed if desired.

## Data Insights
* We can see from the summary statistics that our data is highly skewed with most of the extreme data below the median. The kurtosis helps us determine that our data is heavily tailed in the negative returns.  If we look at all the data that we observed there are only 279 instances where there was no change in direction which indicates that heating oil is a volatile commodity.  We can see from the graphs that there is extreme volatility in both the early 1990’s and 2000.  The extreme fluctuation in the early 1990’s was due to the invasion of Kuwait followed by the Gulf War which created a major supply shock that led to a sharp increase in price of oil.  In 2000 the high fluctuation may have been due to a decision by OPEC members to curb production in 1999 to reverse the preceding years decline in prices. 
* The data also shows volatility in 1990-1991 and in 2000 which were both recessionary periods.

## Business Remarks
* We used a model that fit the gamma distribution which allows for the skew and the heavy tail that is in our data set.  This model is especially useful for time-sensitive material.  To help combat our exposure of variable costs we would need to put a procedure in place that requires upper managements approval when there are costs rise above our tolerable rate of 3.6.

## References

1. Skewness - https://en.wikipedia.org/wiki/Skewness
2. Kurtosis - https://www.investopedia.com/terms/k/kurtosis.asp